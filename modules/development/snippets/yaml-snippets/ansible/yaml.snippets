snippet ansible-playbook "Complete Ansible playbook template with common structure"
---
# ${1:Playbook Name}
# Description: ${2:Description of the playbook}
# Author: ${3:Your Name}
# Created: ${CURRENT_YEAR}-${CURRENT_MONTH}-${CURRENT_DATE}

- name: ${4:Main play for configuring servers}
  hosts: ${5:all}
  become: ${6:yes}
  become_user: ${7:root}
  gather_facts: ${8:yes}

  vars:
    # Global variables
    app_user: ${9:appuser}
    app_group: ${10:appgroup}
    app_directory: ${11:/opt/myapp}
    app_version: ${12:1.0.0}

  vars_files:
    - ${13:vars/main.yml}
    - ${14:vars/secrets.yml}

  tasks:
    - name: ${15:Update apt cache}
      apt:
        update_cache: yes
        cache_valid_time: 3600
      when: ansible_os_family == 'Debian'

    - name: ${16:Install required packages}
      package:
        name: "{{ item }}"
        state: present
      loop:
        - nginx
        - python3
        - python3-pip
        - git

    - name: ${17:Create application user}
      user:
        name: "{{ app_user }}"
        group: "{{ app_group }}"
        shell: /bin/bash
        create_home: yes
        home: "/home/{{ app_user }}"

    - name: ${18:Create application directory}
      file:
        path: "{{ app_directory }}"
        state: directory
        owner: "{{ app_user }}"
        group: "{{ app_group }}"
        mode: '0755'

  handlers:
    - name: ${19:restart nginx}
      service:
        name: nginx
        state: restarted

    - name: ${20:reload systemd}
      systemd:
        daemon_reload: yes
$0
endsnippet

snippet ansible-role-structure "Complete Ansible role directory structure with template files"
role_name/
├── defaults/
│   └── main.yml
├── files/
├── handlers/
│   └── main.yml
├── meta/
│   └── main.yml
├── tasks/
│   └── main.yml
├── templates/
├── tests/
│   ├── inventory
│   └── test.yml
├── vars/
│   └── main.yml
└── README.md

# ${1:Role Name}
# Description: ${2:Role description}

## Default Variables (defaults/main.yml):
---
# Default variables for ${1:Role Name}
app_port: ${3:8080}
app_user: ${4:appuser}
app_install_dir: ${5:/opt/app}
app_version: ${6:1.0.0}

## Main Tasks (tasks/main.yml):
---
# Tasks for ${1:Role Name}

- name: ${7:Install required packages}
  package:
    name: "{{ item }}"
    state: present
  loop: "{{ required_packages }}"
  tags:
    - packages
    - install

- name: ${8:Create application user}
  user:
    name: "{{ app_user }}"
    state: present
    system: yes
    create_home: no
  tags:
    - user
    - setup

## Handlers (handlers/main.yml):
---
# Handlers for ${1:Role Name}

- name: ${9:restart application}
  systemd:
    name: "{{ app_service_name }}"
    state: restarted
    daemon_reload: yes

## Meta Information (meta/main.yml):
---
galaxy_info:
  author: ${10:Your Name}
  description: ${1:Role Name}
  company: ${11:Your Company}
  license: ${12:MIT}
  min_ansible_version: ${13:2.9}
  platforms:
    - name: Ubuntu
      versions:
        - ${14:focal}
        - ${15:jammy}
    - name: Debian
      versions:
        - ${16:10}
        - ${17:11}
  galaxy_tags:
    - ${18:web}
    - ${19:deployment}
    - ${20:system}

dependencies: []
$0
endsnippet

snippet ansible-nginx-setup "Nginx web server configuration playbook"
---
# Nginx Web Server Setup

- name: ${1:Configure Nginx web server}
  hosts: ${2:webservers}
  become: yes

  vars:
    nginx_user: ${3:www-data}
    nginx_worker_processes: ${4:auto}
    nginx_worker_connections: ${5:1024}
    nginx_keepalive_timeout: ${6:65}
    server_name: ${7:example.com}
    server_root: ${8:/var/www/html}
    ssl_enabled: ${9:false}
    ssl_cert_path: ${10:/etc/ssl/certs/ssl-cert-snakeoil.pem}
    ssl_key_path: ${11:/etc/ssl/private/ssl-cert-snakeoil.key}

  tasks:
    - name: ${12:Install Nginx}
      package:
        name: nginx
        state: present

    - name: ${13:Create web directory}
      file:
        path: "{{ server_root }}"
        state: directory
        owner: "{{ nginx_user }}"
        group: "{{ nginx_user }}"
        mode: '0755'

    - name: ${14:Configure Nginx}
      template:
        src: nginx.conf.j2
        dest: /etc/nginx/nginx.conf
        owner: root
        group: root
        mode: '0644'
      notify: reload nginx

    - name: ${15:Create site configuration}
      template:
        src: site.conf.j2
        dest: /etc/nginx/sites-available/{{ server_name }}
        owner: root
        group: root
        mode: '0644'
      notify: reload nginx

    - name: ${16:Enable site}
      file:
        src: /etc/nginx/sites-available/{{ server_name }}
        dest: /etc/nginx/sites-enabled/{{ server_name }}
        state: link
      notify: reload nginx

    - name: ${17:Set up SSL (if enabled)}
      block:
        - name: Install SSL certificates
          copy:
            src: "{{ item.src }}"
            dest: "{{ item.dest }}"
            owner: root
            group: root
            mode: '0644'
          with_items:
            - { src: 'files/ssl/cert.pem', dest: '{{ ssl_cert_path }}' }
            - { src: 'files/ssl/key.pem', dest: '{{ ssl_key_path }}' }
      when: ssl_enabled

    - name: ${18:Configure firewall}
      ufw:
        rule: allow
        port: "{{ '443' if ssl_enabled else '80' }}"
        proto: tcp
      when: ansible_os_family == 'Debian'

  handlers:
    - name: reload nginx
      systemd:
        name: nginx
        state: reloaded

    - name: restart nginx
      systemd:
        name: nginx
        state: restarted
$0
endsnippet

snippet ansible-postgres-setup "PostgreSQL database server setup playbook"
---
# PostgreSQL Database Setup

- name: ${1:Install and configure PostgreSQL}
  hosts: ${2:databases}
  become: yes

  vars:
    postgresql_version: ${3:14}
    postgresql_data_dir: ${4:/var/lib/postgresql/{{ postgresql_version }}/main}
    postgresql_listen_addresses: ${5:'localhost'}
    postgresql_port: ${6:5432}
    postgresql_max_connections: ${7:100}
    postgresql_shared_buffers: ${8:128MB}
    databases:
      - name: ${9:app_db}
        owner: ${10:app_user}
        encoding: ${11:UTF8}
        template: ${12:template0}
        lc_collate: ${13:en_US.UTF-8}
        lc_ctype: ${14:en_US.UTF-8}
    users:
      - name: ${15:app_user}
        password: ${16:changeme}
        encrypted: ${17:yes}
        privileges:
          - "{{ databases[0].name }}:ALL"

  tasks:
    - name: ${18:Add PostgreSQL repository}
      apt_repository:
        repo: "deb https://apt.postgresql.org/pub/repos/apt {{ ansible_distribution_release }}-pgdg main"
        state: present
      when: ansible_os_family == 'Debian'

    - name: ${19:Install PostgreSQL}
      package:
        name: "postgresql-{{ postgresql_version }}"
        state: present

    - name: ${20:Configure PostgreSQL}
      template:
        src: postgresql.conf.j2
        dest: "/etc/postgresql/{{ postgresql_version }}/main/postgresql.conf"
        owner: postgres
        group: postgres
        mode: '0644'
      notify: restart postgresql

    - name: ${21:Configure pg_hba.conf}
      template:
        src: pg_hba.conf.j2
        dest: "/etc/postgresql/{{ postgresql_version }}/main/pg_hba.conf"
        owner: postgres
        group: postgres
        mode: '0640'
      notify: reload postgresql

    - name: ${22:Ensure PostgreSQL service is running}
      systemd:
        name: "postgresql@{{ postgresql_version }}-main"
        state: started
        enabled: yes

    - name: ${23:Create PostgreSQL databases}
      postgresql_db:
        name: "{{ item.name }}"
        owner: "{{ item.owner }}"
        encoding: "{{ item.encoding }}"
        template: "{{ item.template }}"
        lc_collate: "{{ item.lc_collate }}"
        lc_ctype: "{{ item.lc_ctype }}"
      loop: "{{ databases }}"

    - name: ${24:Create PostgreSQL users}
      postgresql_user:
        name: "{{ item.name }}"
        password: "{{ item.password }}"
        encrypted: "{{ item.encrypted }}"
        role_attr_flags: "{{ item.role_attr_flags | default('CREATEDB,NOLOGIN') }}"
      loop: "{{ users }}"

    - name: ${25:Grant privileges to users}
      postgresql_privs:
        database: "{{ item.0 }}"
        roles: "{{ item.1.name }}"
        privs: "{{ item.1.privs }}"
        type: table
        grant_option: no
      with_subelements:
        - "{{ databases }}"
        - users

    - name: ${26:Configure firewall for PostgreSQL}
      ufw:
        rule: allow
        port: "{{ postgresql_port }}"
        proto: tcp
        src: ${27:192.168.1.0/24}
      when: ansible_os_family == 'Debian'

  handlers:
    - name: restart postgresql
      systemd:
        name: "postgresql@{{ postgresql_version }}-main"
        state: restarted

    - name: reload postgresql
      systemd:
        name: "postgresql@{{ postgresql_version }}-main"
        state: reloaded
$0
endsnippet

snippet ansible-docker-setup "Docker installation and configuration playbook"
---
# Docker Installation and Configuration

- name: ${1:Install and configure Docker}
  hosts: ${2:all}
  become: yes

  vars:
    docker_users:
      - ${3:ansible-user}
      - ${4:app-user}
    docker_version: ${5:20.10}
    docker_storage_driver: ${6:overlay2}
    docker_log_driver: ${7:json-file}
    docker_log_opts:
      max-size: ${8:10m}
      max-file: ${9:'3'}
    docker_compose_version: ${10:1.29.2}

  tasks:
    - name: ${11:Install dependencies}
      package:
        name: "{{ item }}"
        state: present
      loop:
        - apt-transport-https
        - ca-certificates
        - curl
        - gnupg
        - lsb-release
        - software-properties-common
      when: ansible_os_family == 'Debian'

    - name: ${12:Add Docker GPG key}
      apt_key:
        url: https://download.docker.com/linux/{{ ansible_distribution | lower }}/gpg
        state: present
      when: ansible_os_family == 'Debian'

    - name: ${13:Add Docker repository}
      apt_repository:
        repo: "deb [arch=amd64] https://download.docker.com/linux/{{ ansible_distribution | lower }} {{ ansible_distribution_release }} stable"
        state: present
        update_cache: yes
      when: ansible_os_family == 'Debian'

    - name: ${14:Install Docker}
      package:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
        state: present

    - name: ${15:Install Docker Compose}
      get_url:
        url: "https://github.com/docker/compose/releases/download/{{ docker_compose_version }}/docker-compose-$(uname -s)-$(uname -m)"
        dest: /usr/local/bin/docker-compose
        mode: '0755'

    - name: ${16:Create docker group}
      group:
        name: docker
        state: present

    - name: ${17:Add users to docker group}
      user:
        name: "{{ item }}"
        groups: docker
        append: yes
      loop: "{{ docker_users }}"

    - name: ${18:Configure Docker daemon}
      copy:
        content: |
          {
            "log-driver": "{{ docker_log_driver }}",
            "log-opts": {
              "max-size": "{{ docker_log_opts['max-size'] }}",
              "max-file": "{{ docker_log_opts['max-file'] }}"
            },
            "storage-driver": "{{ docker_storage_driver }}",
            "live-restore": true
          }
        dest: /etc/docker/daemon.json
        owner: root
        group: root
        mode: '0644'
      notify: restart docker

    - name: ${19:Start and enable Docker service}
      systemd:
        name: docker
        state: started
        enabled: yes
        daemon_reload: yes

    - name: ${20:Create Docker networks}
      docker_network:
        name: "{{ item.name }}"
        driver: "{{ item.driver | default('bridge') }}"
        ipam_config:
          - subnet: "{{ item.subnet }}"
        state: present
      loop:
        - { name: frontend, subnet: 172.20.0.0/16 }
        - { name: backend, subnet: 172.21.0.0/16 }

    - name: ${21:Set up Docker cleanup cron job}
      cron:
        name: "Docker system prune"
        special_time: daily
        job: "docker system prune -f"
        user: root

  handlers:
    - name: restart docker
      systemd:
        name: docker
        state: restarted

  post_tasks:
    - name: ${22:Verify Docker installation}
      command: docker --version
      register: docker_version_result

    - name: ${23:Display Docker version}
      debug:
        msg: "Docker installed: {{ docker_version_result.stdout }}"
$0
endsnippet

snippet ansible-k8s-setup "Kubernetes cluster setup with kubeadm"
---
# Kubernetes Cluster Setup with kubeadm

- name: ${1:Setup Kubernetes master node}
  hosts: ${2:k8s_master}
  become: yes

  vars:
    kubernetes_version: ${3:1.26.0-00}
    pod_network_cidr: ${4:10.244.0.0/16}
    service_cidr: ${5:10.96.0.0/12}
    cluster_name: ${6:my-cluster}
    advertise_address: ${7:"{{ ansible_default_ipv4.address }}"}

  tasks:
    - name: ${8:Disable swap}
      shell: swapoff -a
      ignore_errors: yes

    - name: ${9:Remove swap from /etc/fstab}
      lineinfile:
        path: /etc/fstab
        regexp: '^.*swap.*$'
        state: absent

    - name: ${10:Load kernel modules}
      modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - br_netfilter
        - overlay

    - name: ${11:Configure sysctl parameters}
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { name: net.bridge.bridge-nf-call-iptables, value: 1 }
        - { name: net.bridge.bridge-nf-call-ip6tables, value: 1 }
        - { name: net.ipv4.ip_forward, value: 1 }

    - name: ${12:Add Kubernetes repository}
      apt_repository:
        repo: deb https://apt.kubernetes.io/ kubernetes-xenial main
        state: present
        filename: kubernetes
      when: ansible_os_family == 'Debian'

    - name: ${13:Add Kubernetes GPG key}
      apt_key:
        url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
        state: present
      when: ansible_os_family == 'Debian'

    - name: ${14:Install kubeadm, kubelet and kubectl}
      package:
        name:
          - kubeadm={{ kubernetes_version }}
          - kubelet={{ kubernetes_version }}
          - kubectl={{ kubernetes_version }}
        state: present
        update_cache: yes

    - name: ${15:Hold Kubernetes packages at current version}
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubeadm
        - kubelet
        - kubectl

    - name: ${16:Initialize Kubernetes cluster}
      command: >
        kubeadm init
        --pod-network-cidr={{ pod_network_cidr }}
        --service-cidr={{ service_cidr }}
        --control-plane-endpoint={{ advertise_address }}
        --apiserver-advertise-address={{ advertise_address }}
        --node-name={{ inventory_hostname }}
        --ignore-preflight-errors=all
      register: kubeadm_init

    - name: ${17:Create .kube directory}
      file:
        path: $HOME/.kube
        state: directory
        mode: '0750'

    - name: ${18:Copy kubeconfig to user's home}
      copy:
        src: /etc/kubernetes/admin.conf
        dest: $HOME/.kube/config
        remote_src: yes
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    - name: ${19:Deploy Flannel network plugin}
      command: kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
      when: kubeadm_init.rc == 0

- name: ${20:Setup Kubernetes worker nodes}
  hosts: ${21:k8s_workers}
  become: yes

  vars:
    join_command: "{{ hostvars[groups['k8s_master'][0]].kubeadm_init.stdout_lines[-2] }}"

  tasks:
    - name: ${22:Run kubeadm join command}
      command: "{{ join_command }}"
      when: join_command is defined

- name: ${23:Post-installation configuration}
  hosts: ${24:k8s_master}
  become: yes

  tasks:
    - name: ${25:Remove taint from master node}
      command: kubectl taint nodes --all node-role.kubernetes.io/master-
      when: kubeadm_init.rc == 0

    - name: ${26:Install metrics server}
      command: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

    - name: ${27:Install Kubernetes dashboard}
      command: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

    - name: ${28:Create admin service account}
      k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: admin-user
            namespace: kubernetes-dashboard

    - name: ${29:Create cluster role binding}
      k8s:
        state: present
        definition:
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: admin-user
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: cluster-admin
          subjects:
            - kind: ServiceAccount
              name: admin-user
              namespace: kubernetes-dashboard
$0
endsnippet

snippet ansible-security "Linux server security hardening playbook"
---
# Linux Server Security Hardening

- name: ${1:Security hardening for Linux servers}
  hosts: ${2:all}
  become: yes

  vars:
    security_admin_users:
      - ${3:admin}
      - ${4:ansible}
    ssh_port: ${5:2222}
    fail2ban_enabled: ${6:true}
    firewall_enabled: ${7:true}
    allowed_ports:
      - ${8:22}
      - ${9:80}
      - ${10:443}

  tasks:
    - name: ${11:Update all packages to latest versions}
      apt:
        upgrade: dist
        update_cache: yes
        cache_valid_time: 3600
      when: ansible_os_family == 'Debian'

    - name: ${12:Remove unnecessary packages}
      package:
        name: "{{ item }}"
        state: absent
      loop:
        - telnet
        - rsh-server
        - rsh-client
        - nis
        - yp-tools
        - talk
        - tftp

    - name: ${13:Configure SSH security}
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: "^{{ item.regex }}$"
        line: "{{ item.line }}"
        state: present
      loop:
        - { regex: '^#?Port', line: 'Port {{ ssh_port }}' }
        - { regex: '^#?PermitRootLogin', line: 'PermitRootLogin no' }
        - { regex: '^#?PasswordAuthentication', line: 'PasswordAuthentication no' }
        - { regex: '^#?PubkeyAuthentication', line: 'PubkeyAuthentication yes' }
        - { regex: '^#?ChallengeResponseAuthentication', line: 'ChallengeResponseAuthentication no' }
        - { regex: '^#?UsePAM', line: 'UsePAM yes' }
        - { regex: '^#?AllowUsers', line: 'AllowUsers {{ security_admin_users | join(" ") }}' }
        - { regex: '^#?ClientAliveInterval', line: 'ClientAliveInterval 300' }
        - { regex: '^#?ClientAliveCountMax', line: 'ClientAliveCountMax 2' }
        - { regex: '^#?MaxAuthTries', line: 'MaxAuthTries 3' }
      notify: restart ssh

    - name: ${14:Set up SSH keys for admin users}
      authorized_key:
        user: "{{ item }}"
        state: present
        key: "{{ lookup('file', 'files/ssh_keys/{{ item }}.pub') }}"
      loop: "{{ security_admin_users }}"

    - name: ${15:Configure firewall (UFW)}
      ufw:
        state: enabled
        policy: deny
        direction: incoming
        logging: on
      when: firewall_enabled and ansible_os_family == 'Debian'

    - name: ${16:Allow specific ports in firewall}
      ufw:
        rule: allow
        port: "{{ item }}"
        proto: tcp
      loop: "{{ allowed_ports }}"
      when: firewall_enabled and ansible_os_family == 'Debian'

    - name: ${17:Install and configure fail2ban}
      block:
        - name: Install fail2ban
          package:
            name: fail2ban
            state: present

        - name: Configure fail2ban
          template:
            src: fail2ban.jail.local.j2
            dest: /etc/fail2ban/jail.local
            owner: root
            group: root
            mode: '0644'
          notify: restart fail2ban
      when: fail2ban_enabled

    - name: ${18:Configure sysctl security parameters}
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { name: net.ipv4.conf.all.rp_filter, value: 1 }
        - { name: net.ipv4.conf.default.rp_filter, value: 1 }
        - { name: net.ipv4.tcp_syncookies, value: 1 }
        - { name: net.ipv4.ip_forward, value: 0 }
        - { name: net.ipv6.conf.all.forwarding, value: 0 }
        - { name: net.ipv4.conf.all.send_redirects, value: 0 }
        - { name: net.ipv4.conf.default.send_redirects, value: 0 }
        - { name: net.ipv4.conf.all.accept_redirects, value: 0 }
        - { name: net.ipv6.conf.all.accept_redirects, value: 0 }
        - { name: net.ipv4.conf.all.secure_redirects, value: 0 }

    - name: ${19:Configure PAM security}
      lineinfile:
        path: /etc/pam.d/common-password
        regexp: '^password.*pam_unix.so.*'
        line: 'password        [success=1 default=ignore]      pam_unix.so obscure sha512 minlen=12 remember=5'
      when: ansible_os_family == 'Debian'

    - name: ${20:Set password expiration policy}
      lineinfile:
        path: /etc/login.defs
        regexp: "^{{ item.regex }}"
        line: "{{ item.line }}"
      loop:
        - { regex: '^PASS_MAX_DAYS', line: 'PASS_MAX_DAYS   90' }
        - { regex: '^PASS_MIN_DAYS', line: 'PASS_MIN_DAYS   7' }
        - { regex: '^PASS_WARN_AGE', line: 'PASS_WARN_AGE   14' }

    - name: ${21:Install and configure auditd}
      block:
        - name: Install auditd
          package:
            name: auditd
            state: present

        - name: Configure audit rules
          copy:
            src: files/audit/audit.rules
            dest: /etc/audit/rules.d/audit.rules
            owner: root
            group: root
            mode: '0640'
          notify: restart auditd

    - name: ${22:Set file permissions}
      file:
        path: "{{ item.path }}"
        owner: "{{ item.owner | default('root') }}"
        group: "{{ item.group | default('root') }}"
        mode: "{{ item.mode }}"
      loop:
        - { path: /etc/passwd, mode: '0644' }
        - { path: /etc/shadow, mode: '0640' }
        - { path: /etc/group, mode: '0644' }
        - { path: /etc/gshadow, mode: '0640' }

  handlers:
    - name: restart ssh
      systemd:
        name: ssh
        state: restarted

    - name: restart fail2ban
      systemd:
        name: fail2ban
        state: restarted

    - name: restart auditd
      systemd:
        name: auditd
        state: restarted
$0
endsnippet

snippet ansible-monitoring "Prometheus + Grafana monitoring stack deployment"
---
# Monitoring Stack with Prometheus and Grafana

- name: ${1:Deploy monitoring stack}
  hosts: ${2:monitoring}
  become: yes

  vars:
    prometheus_version: ${3:2.45.0}
    grafana_version: ${4:9.5.2}
    node_exporter_version: ${5:1.6.0}
    alertmanager_version: ${6:0.25.0}
    monitoring_user: ${7:monitoring}
    prometheus_data_dir: ${8:/var/lib/prometheus}
    grafana_data_dir: ${9:/var/lib/grafana}
    alertmanager_data_dir: ${10:/var/lib/alertmanager}

  tasks:
    - name: ${11:Create monitoring user}
      user:
        name: "{{ monitoring_user }}"
        system: yes
        shell: /usr/sbin/nologin
        create_home: no

    - name: ${12:Install Prometheus}
      block:
        - name: Download Prometheus
          get_url:
            url: "https://github.com/prometheus/prometheus/releases/download/v{{ prometheus_version }}/prometheus-{{ prometheus_version }}.linux-amd64.tar.gz"
            dest: /tmp/prometheus.tar.gz

        - name: Extract Prometheus
          unarchive:
            src: /tmp/prometheus.tar.gz
            dest: /tmp
            remote_src: yes

        - name: Install Prometheus files
          copy:
            src: "/tmp/prometheus-{{ prometheus_version }}.linux-amd64/{{ item }}"
            dest: "/usr/local/bin/{{ item }}"
            mode: '0755'
            owner: root
            group: root
          loop:
            - prometheus
            - promtool

        - name: Create Prometheus directories
          file:
            path: "{{ item }}"
            state: directory
            owner: "{{ monitoring_user }}"
            group: "{{ monitoring_user }}"
            mode: '0755'
          loop:
            - "{{ prometheus_data_dir }}"
            - /etc/prometheus
            - /var/log/prometheus

        - name: Configure Prometheus
          template:
            src: prometheus.yml.j2
            dest: /etc/prometheus/prometheus.yml
            owner: "{{ monitoring_user }}"
            group: "{{ monitoring_user }}"
            mode: '0644'
          notify: restart prometheus

        - name: Create Prometheus systemd service
          template:
            src: prometheus.service.j2
            dest: /etc/systemd/system/prometheus.service
            owner: root
            group: root
            mode: '0644'
          notify:
            - reload systemd
            - restart prometheus

    - name: ${13:Install Node Exporter}
      block:
        - name: Download Node Exporter
          get_url:
            url: "https://github.com/prometheus/node_exporter/releases/download/v{{ node_exporter_version }}/node_exporter-{{ node_exporter_version }}.linux-amd64.tar.gz"
            dest: /tmp/node_exporter.tar.gz

        - name: Install Node Exporter
          unarchive:
            src: /tmp/node_exporter.tar.gz
            dest: /usr/local/bin
            remote_src: yes
            extra_opts: [--strip-components=1]
            creates: /usr/local/bin/node_exporter

        - name: Create Node Exporter systemd service
          template:
            src: node_exporter.service.j2
            dest: /etc/systemd/system/node_exporter.service
            owner: root
            group: root
            mode: '0644'
          notify:
            - reload systemd
            - restart node_exporter

    - name: ${14:Install Grafana}
      block:
        - name: Add Grafana repository
          apt_repository:
            repo: "deb https://packages.grafana.com/oss/deb stable main"
            state: present
            filename: grafana
          when: ansible_os_family == 'Debian'

        - name: Add Grafana GPG key
          apt_key:
            url: https://packages.grafana.com/gpg.key
            state: present
          when: ansible_os_family == 'Debian'

        - name: Install Grafana
          package:
            name: grafana
            state: present
            update_cache: yes

        - name: Configure Grafana
          template:
            src: grafana.ini.j2
            dest: /etc/grafana/grafana.ini
            owner: root
            group: grafana
            mode: '0640'
          notify: restart grafana

        - name: Install Grafana plugins
          grafana_plugin:
            name: "{{ item }}"
            grafana_dir: /usr/share/grafana
            state: present
          loop:
            - grafana-piechart-panel
            - grafana-clock-panel
            - grafana-worldmap-panel

        - name: Configure Grafana datasources
          grafana_datasource:
            grafana_url: http://localhost:3000
            grafana_user: admin
            grafana_password: admin
            name: Prometheus
            ds_type: prometheus
            url: http://localhost:9090
            access: proxy
            state: present

    - name: ${15:Install Alertmanager}
      block:
        - name: Download Alertmanager
          get_url:
            url: "https://github.com/prometheus/alertmanager/releases/download/v{{ alertmanager_version }}/alertmanager-{{ alertmanager_version }}.linux-amd64.tar.gz"
            dest: /tmp/alertmanager.tar.gz

        - name: Install Alertmanager
          unarchive:
            src: /tmp/alertmanager.tar.gz
            dest: /tmp
            remote_src: yes

        - name: Install Alertmanager files
          copy:
            src: "/tmp/alertmanager-{{ alertmanager_version }}.linux-amd64/{{ item }}"
            dest: "/usr/local/bin/{{ item }}"
            mode: '0755'
            owner: root
            group: root
          loop:
            - alertmanager
            - amtool

        - name: Create Alertmanager directories
          file:
            path: "{{ item }}"
            state: directory
            owner: "{{ monitoring_user }}"
            group: "{{ monitoring_user }}"
            mode: '0755'
          loop:
            - "{{ alertmanager_data_dir }}"
            - /etc/alertmanager

        - name: Configure Alertmanager
          template:
            src: alertmanager.yml.j2
            dest: /etc/alertmanager/alertmanager.yml
            owner: "{{ monitoring_user }}"
            group: "{{ monitoring_user }}"
            mode: '0644'
          notify: restart alertmanager

        - name: Create Alertmanager systemd service
          template:
            src: alertmanager.service.j2
            dest: /etc/systemd/system/alertmanager.service
            owner: root
            group: root
            mode: '0644'
          notify:
            - reload systemd
            - restart alertmanager

    - name: ${16:Start and enable services}
      systemd:
        name: "{{ item }}"
        state: started
        enabled: yes
        daemon_reload: yes
      loop:
        - prometheus
        - node_exporter
        - grafana-server
        - alertmanager

    - name: ${17:Configure firewall for monitoring ports}
      ufw:
        rule: allow
        port: "{{ item }}"
        proto: tcp
      loop:
        - 9090  # Prometheus
        - 3000  # Grafana
        - 9093  # Alertmanager
        - 9100  # Node Exporter
      when: ansible_os_family == 'Debian'

  handlers:
    - name: reload systemd
      systemd:
        daemon_reload: yes

    - name: restart prometheus
      systemd:
        name: prometheus
        state: restarted

    - name: restart node_exporter
      systemd:
        name: node_exporter
        state: restarted

    - name: restart grafana
      systemd:
        name: grafana-server
        state: restarted

    - name: restart alertmanager
      systemd:
        name: alertmanager
        state: restarted
$0
endsnippet

snippet ansible-aws-infra "AWS infrastructure provisioning playbook (VPC, EC2, RDS, ALB)"
---
# AWS Infrastructure Provisioning with Ansible

- name: ${1:Provision AWS VPC and Networking}
  hosts: ${2:localhost}
  connection: local
  gather_facts: no

  vars:
    aws_region: ${3:us-east-1}
    vpc_cidr: ${4:10.0.0.0/16}
    vpc_name: ${5:my-vpc}
    public_subnet_cidr: ${6:10.0.1.0/24}
    private_subnet_cidr: ${7:10.0.2.0/24}
    availability_zone: ${8:us-east-1a}

  tasks:
    - name: ${9:Create VPC}
      ec2_vpc_net:
        name: "{{ vpc_name }}"
        cidr_block: "{{ vpc_cidr }}"
        region: "{{ aws_region }}"
        tenancy: default
        dns_support: yes
        dns_hostnames: yes
        state: present
      register: vpc

    - name: ${10:Create Internet Gateway}
      ec2_vpc_igw:
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ aws_region }}"
        state: present
      register: igw

    - name: ${11:Create public subnet}
      ec2_vpc_subnet:
        vpc_id: "{{ vpc.vpc.id }}"
        cidr: "{{ public_subnet_cidr }}"
        az: "{{ availability_zone }}"
        region: "{{ aws_region }}"
        map_public: yes
        state: present
      register: public_subnet

    - name: ${12:Create private subnet}
      ec2_vpc_subnet:
        vpc_id: "{{ vpc.vpc.id }}"
        cidr: "{{ private_subnet_cidr }}"
        az: "{{ availability_zone }}"
        region: "{{ aws_region }}"
        state: present
      register: private_subnet

    - name: ${13:Create route table for public subnet}
      ec2_vpc_route_table:
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ aws_region }}"
        subnets:
          - "{{ public_subnet.subnet.id }}"
        routes:
          - dest: 0.0.0.0/0
            gateway_id: "{{ igw.gateway_id }}"
        state: present
      register: public_route_table

- name: ${14:Provision AWS Security Groups}
  hosts: ${15:localhost}
  connection: local
  gather_facts: no

  tasks:
    - name: ${16:Create web security group}
      ec2_group:
        name: ${17:web-sg}
        description: ${18:Security group for web servers}
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ aws_region }}"
        rules:
          - proto: tcp
            from_port: 22
            to_port: 22
            cidr_ip: 0.0.0.0/0
            rule_desc: SSH access
          - proto: tcp
            from_port: 80
            to_port: 80
            cidr_ip: 0.0.0.0/0
            rule_desc: HTTP access
          - proto: tcp
            from_port: 443
            to_port: 443
            cidr_ip: 0.0.0.0/0
            rule_desc: HTTPS access
        rules_egress:
          - proto: all
            cidr_ip: 0.0.0.0/0
        state: present

    - name: ${19:Create database security group}
      ec2_group:
        name: ${20:db-sg}
        description: ${21:Security group for database servers}
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ aws_region }}"
        rules:
          - proto: tcp
            from_port: 3306
            to_port: 3306
            group_id: "{{ ec2_groups['web-sg'].group_id }}"
            rule_desc: MySQL access from web servers
        state: present

- name: ${22:Provision EC2 Instances}
  hosts: ${23:localhost}
  connection: local
  gather_facts: no

  vars:
    key_name: ${24:my-key-pair}
    instance_type: ${25:t3.micro}
    ami_id: ${26:ami-0c55b159cbfafe1f0}  # Amazon Linux 2

  tasks:
    - name: ${27:Create web server instance}
      ec2_instance:
        name: ${28:web-server-01}
        key_name: "{{ key_name }}"
        vpc_subnet_id: "{{ public_subnet.subnet.id }}"
        security_group: ${29:web-sg}
        network:
          assign_public_ip: yes
        instance_type: "{{ instance_type }}"
        image_id: "{{ ami_id }}"
        region: "{{ aws_region }}"
        volumes:
          - device_name: /dev/xvda
            volume_type: gp3
            volume_size: 20
            delete_on_termination: yes
        tags:
          Environment: ${30:Production}
          Role: ${31:WebServer}
        state: present
      register: web_instance

    - name: ${32:Create database instance}
      ec2_instance:
        name: ${33:db-server-01}
        key_name: "{{ key_name }}"
        vpc_subnet_id: "{{ private_subnet.subnet.id }}"
        security_group: ${34:db-sg}
        instance_type: "{{ instance_type }}"
        image_id: "{{ ami_id }}"
        region: "{{ aws_region }}"
        volumes:
          - device_name: /dev/xvda
            volume_type: gp3
            volume_size: 50
            delete_on_termination: yes
          - device_name: /dev/xvdf
            volume_type: gp3
            volume_size: 100
            delete_on_termination: yes
        tags:
          Environment: ${30:Production}
          Role: ${35:Database}
        state: present
      register: db_instance

- name: ${36:Configure Load Balancer}
  hosts: ${37:localhost}
  connection: local
  gather_facts: no

  tasks:
    - name: ${38:Create Application Load Balancer}
      elb_application_lb:
        name: ${39:web-alb}
        subnets: "{{ public_subnet.subnet.id }}"
        security_groups: "{{ ec2_groups['web-sg'].group_id }}"
        region: "{{ aws_region }}"
        scheme: internet-facing
        state: present
        listeners:
          - Protocol: HTTP
            Port: 80
            DefaultActions:
              - Type: forward
                TargetGroupName: web-tg
      register: alb

    - name: ${40:Create target group}
      elb_target_group:
        name: ${41:web-tg}
        protocol: HTTP
        port: 80
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ aws_region }}"
        health_check_path: /
        health_check_port: 80
        health_check_protocol: HTTP
        target_type: instance
        state: present

    - name: ${42:Register instances with target group}
      elb_target:
        target_group_name: ${41:web-tg}
        target_type: instance
        target_id: "{{ web_instance.instances[0].instance_id }}"
        target_port: 80
        region: "{{ aws_region }}"
        state: present

- name: ${43:Configure RDS Database}
  hosts: ${44:localhost}
  connection: local
  gather_facts: no

  tasks:
    - name: ${45:Create RDS subnet group}
      rds_subnet_group:
        state: present
        name: ${46:my-rds-subnet-group}
        description: ${47:Subnet group for RDS instances}
        region: "{{ aws_region }}"
        subnets:
          - "{{ private_subnet.subnet.id }}"

    - name: ${48:Create RDS instance}
      rds_instance:
        state: present
        name: ${49:myapp-db}
        engine: ${50:mysql}
        engine_version: ${51:8.0}
        db_instance_class: ${52:db.t3.micro}
        username: ${53:admin}
        password: ${54:SecurePass123!}
        allocated_storage: ${55:20}
        storage_type: ${56:gp2}
        backup_retention_period: ${57:7}
        multi_zone: ${58:no}
        publicly_accessible: ${59:no}
        vpc_security_groups: "{{ ec2_groups['db-sg'].group_id }}"
        db_subnet_group_name: ${46:my-rds-subnet-group}
        region: "{{ aws_region }}"
        tags:
          Environment: ${60:Production}
          Application: ${61:MyApp}
$0
endsnippet

snippet ansible-dynamic-inventory "Dynamic inventory configurations for various cloud providers"
# Dynamic Inventory Configuration

# ${1:AWS EC2 Dynamic Inventory}
plugin: aws_ec2
regions:
  - ${2:us-east-1}
  - ${3:us-west-2}
filters:
  # Filter by instance state
  instance-state-name:
    - running
keyed_groups:
  # Add hosts to tag_Name_{Value} groups
  - key: tags
    prefix: tag
  # Add hosts to security group groups
  - key: security_groups|json_query('[].group_name')
    prefix: security_group
  # Create groups based on instance type
  - key: instance_type
    prefix: type
hostnames:
  # Use public DNS name if available, otherwise private IP
  - tag:Name
  - dns-name
  - private-ip-address
compose:
  # Custom ansible_host
  ansible_host: public_dns_name
  # Fallback to private IP if no public DNS
  ansible_host: private_ip_address

# ${4:Azure Dynamic Inventory}
plugin: azure_rm
include_vm_resource_groups:
  - ${5:my-resource-group}
auth_source: ${6:auto}
keyed_groups:
  - key: tags
    prefix: tag
  - key: resource_group
    prefix: rg
  - key: location
    prefix: location

# ${7:GCP Dynamic Inventory}
plugin: gcp_compute
projects:
  - ${8:my-gcp-project}
auth_kind: ${9:serviceaccount}
service_account_file: ${10:/path/to/service-account.json}
scopes:
  - https://www.googleapis.com/auth/compute.readonly
keyed_groups:
  - key: labels
    prefix: label
  - key: zone
    prefix: zone

# ${11:Docker Dynamic Inventory}
plugin: docker_containers
connection_type: ${12:socket}
verbose_output: ${13:true}
keyed_groups:
  - key: labels
    prefix: label

# ${14:Custom Filters and Groups}
# Group instances by tag Environment
groups:
  webservers: "'webserver' in (tags|list)"
  databases: "'database' in (tags|list)"
  production: "'Environment' in tags and tags.Environment == 'production'"
  staging: "'Environment' in tags and tags.Environment == 'staging'"

# ${15:Example ansible.cfg configuration}
[defaults]
inventory = ./inventory/
host_key_checking = False
retry_files_enabled = False
stdout_callback = yaml
callback_whitelist = profile_tasks, timer

[inventory]
enable_plugins = aws_ec2, azure_rm, gcp_compute, docker_containers, auto, yaml, ini

# ${16:Example playbook using dynamic inventory}
---
# Playbook using dynamic inventory
- name: Configure web servers from AWS
  hosts: tag_Environment_production
  gather_facts: yes
  tasks:
    - name: Install nginx on web servers
      apt:
        name: nginx
        state: present
      when: ansible_os_family == 'Debian'

- name: Configure database servers
  hosts: tag_Role_database
  tasks:
    - name: Install PostgreSQL
      apt:
        name: postgresql
        state: present

# ${17:Using inventory script directly}
#!/bin/bash
# inventory.sh
echo '{'
echo '  "webservers": {'
echo '    "hosts": ['
echo '      "web1.example.com",'
echo '      "web2.example.com"'
echo '    ],'
echo '    "vars": {'
echo '      "ansible_user": "ubuntu"'
echo '    }'
echo '  },'
echo '  "databases": {'
echo '    "hosts": ['
echo '      "db1.example.com"'
echo '    ]'
echo '  }'
echo '}'

# ${18:Python-based dynamic inventory}
#!/usr/bin/env python3
# dynamic_inventory.py
import json
import argparse

def get_inventory():
    return {
        'webservers': {
            'hosts': ['192.168.1.10', '192.168.1.11'],
            'vars': {
                'ansible_user': 'admin',
                'application_port': 8080
            }
        },
        'databases': {
            'hosts': ['192.168.1.20'],
            'vars': {
                'ansible_user': 'postgres',
                'db_port': 5432
            }
        },
        '_meta': {
            'hostvars': {
                '192.168.1.10': {
                    'host_specific_var': 'foo'
                }
            }
        }
    }

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--list', action='store_true')
    parser.add_argument('--host', action='store')
    args = parser.parse_args()

    if args.list:
        print(json.dumps(get_inventory()))
    elif args.host:
        print(json.dumps({}))
$0
endsnippet
